{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, predict and evaluate for COVID-19 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import metrics\n",
    "\n",
    "from helpers.regression import regression as RegressionHelper\n",
    "from helpers.regression import plotDataSortedByActual\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH_DATA_CASES = './data/computed/cases.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "At first we are reading the data computed for this specific task.\n",
    "This includes the number of total cases (and total cases per million) as well as information about the gdp, health care system, tourism and population.\n",
    "We also print a correlation matrix to show our prepared data and how it's connected as well as the parts of the data itself.\n",
    "\n",
    "We also remove all data with missing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_DATA_CASES)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_countries = df.copy()\n",
    "df.drop([\"iso_code\", \"location\", \"total_cases\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Our first approach is to simply try and fit our data to a linear regression.\n",
    "As you can see this fails miserably, because on one hand the scales of our data are completely different, e.g. indicators for the health care system are between 2.2 and 16.8 while GDP per capita is between 661 and 116936.\n",
    "The impact of GDP per capita will be far greater than our health care can be, simply because of their respective scaling.\n",
    "On the other hand our data tries to explain a real pandemic without considering most of the political decisions, we opted to include the lockdown data, but that simply was not enough.\n",
    "This last point is easy to think about in hindsight, but even with more data on politics the current state of research simply does not allow to accurately predict the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"total_cases_per_million\", axis=1)\n",
    "y = df[\"total_cases_per_million\"]\n",
    "\n",
    "RegressionHelper(X, y, LinearRegression(), 0.2, 42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results of our first try are _not great, not terrible_.\n",
    "To improve the linear regression we decided to try out a scaled and a normalized version of the regression.\n",
    "In both tries we also added polynomial features to account for the number of features we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = df.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_scaled = pd.DataFrame(x_scaled, columns=[\"total_cases_per_million\", \"gdp_per_capita\",\n",
    "                                            \"health_expenditure_5y_avg\",\"Population density (people per sq. km of land area)\",\n",
    "                                            \"lockdown_type\", \"International tourism, expenditures (% of total imports)\"])\n",
    "\n",
    "x_normalized = normalize(df.values, axis=0, norm='max')\n",
    "df_normalized = pd.DataFrame(x_normalized, columns=[\"total_cases_per_million\", \"gdp_per_capita\",\n",
    "                                            \"health_expenditure_5y_avg\",\"Population density (people per sq. km of land area)\",\n",
    "                                            \"lockdown_type\", \"International tourism, expenditures (% of total imports)\"])\n",
    "\n",
    "#df_scaled.describe()\n",
    "#df_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = df_scaled.drop(\"total_cases_per_million\", axis=1)\n",
    "y_scaled = df_scaled[\"total_cases_per_million\"]\n",
    "\n",
    "X_normalized = df_normalized.drop(\"total_cases_per_million\", axis=1)\n",
    "y_normalized = df_normalized[\"total_cases_per_million\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly3 = PolynomialFeatures(3)\n",
    "x_poly_scaled = poly3.fit_transform(X_scaled)\n",
    "df_poly_scaled = pd.DataFrame(x_poly_scaled)\n",
    "X_poly_scaled = df_poly_scaled\n",
    "\n",
    "x_poly_normalized = poly3.fit_transform(X_normalized)\n",
    "df_poly_normalized = pd.DataFrame(x_poly_normalized)\n",
    "X_poly_normalized = df_poly_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing regression with scaled and polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RegressionHelper(X_poly_scaled, y_scaled, LinearRegression(), 0.2, 42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing regression with normalized and polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RegressionHelper(X_poly_normalized, y_normalized, LinearRegression(), 0.2, 42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at a visualization of both approaches we can see that normalization and scaling don't differ that much.\n",
    "\n",
    "MAE of around 0.11 means that on average the predicted value differs from the actual one by 0.11.\n",
    "It shows us that the mean distance between the predicted and actual value is pretty high (because we are on a scale of 0 to 1 and most values are between 0 and 0.4).\n",
    "\n",
    "MSE uses the same calculation as MRE with the twist of using squared values of the distance instead of the distance itself.\n",
    "This is to better represent the effect outliers have on the graph.\n",
    "That this score with 0.07 seems lower than MAE is because our values are between 0 and 1, thus squaring leads to smaller values.\n",
    "To make it somehow comparable we can calculate the root of MSE which is much higher (0.27) and fits to the observation that we have some significant outliers.\n",
    "\n",
    "R² is closely related to MSE but not quite the same.\n",
    "It compares the variance of our predicted with the actual values and measures how well the label can be explained by the features.\n",
    "It should be in a range of 0 and 1, where 0 means similar to predicting the average and 1 means perfect explanation.\n",
    "We can see that our R² is negative (around -5) which means, according to sklearn docs, that it's worse than just predicting the average.\n",
    "\n",
    "\n",
    "Normalization is recommended for sparse datasets, standardization for gaussian distributions. We use rescaling because it's the simplest of all methods and in theory should work well for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a closer look at how the new regression performed and compare it to the first try, we can really see a lot of improvement, but also detect some overfitting, because of the number of training datasets and the number of features due to the introduction of 56 polynomial features (with dimension 3).\n",
    "\n",
    "\n",
    "Because of the overfitting we decided to decrease the number of polynomial features by lowering the dimension to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2 = PolynomialFeatures(2)\n",
    "x_poly2_scaled = poly2.fit_transform(X_scaled)\n",
    "df_poly2_scaled = pd.DataFrame(x_poly2_scaled)\n",
    "X_poly2_scaled = df_poly2_scaled\n",
    "\n",
    "[lin_reg, lin_reg_y_test, lin_reg_y_pred] = RegressionHelper(X_poly2_scaled, y_scaled, LinearRegression(), 0.2, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reducing the dimensions to 2 (when applying polynomial features) we have a worse performance on train data, but the prediction for our test data improves (MAE, MSE, RMSE decreased; R² moves towards 0).\n",
    "Obviously we reduced overfitting.\n",
    "\n",
    "As a last resort we decided to remove the outliers of our data, but that also deminishes our chance of reaching our goal, because we remove countries reporting _more accurate_ numbers than others and thus can not really predict how many cases a country of a certain size with certain attributes should have.\n",
    "We will simply have a low number of cases for all countries, because that is more in line with our data.\n",
    "Anyway, the following depicts the data if we'd remove the largest value and get everything closer together.\n",
    "As you can see this does not improve our results, thus we try out another way of predicting the case numbers: Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_largest_value = df.nlargest(columns=\"total_cases_per_million\",n=1).index[0]\n",
    "\n",
    "x_wo_outlier = df.drop(row_largest_value).values\n",
    "x_wo_outlier_scaled = min_max_scaler.fit_transform(x_wo_outlier)\n",
    "df_wo_outlier_scaled = pd.DataFrame(x_wo_outlier_scaled, columns=[\"total_cases_per_million\", \"gdp_per_capita\",\n",
    "                                            \"health_expenditure_5y_avg\",\"Population density (people per sq. km of land area)\",\n",
    "                                            \"lockdown_type\", \"International tourism, expenditures (% of total imports)\"])\n",
    "X_wo_outlier_scaled = df_wo_outlier_scaled.drop(\"total_cases_per_million\", axis=1)\n",
    "y_wo_outlier_scaled = df_wo_outlier_scaled[\"total_cases_per_million\"]\n",
    "\n",
    "poly2 = PolynomialFeatures(2)\n",
    "x_poly2_scaled_wo_outlier = poly2.fit_transform(X_wo_outlier_scaled)\n",
    "df_poly2_scaled_wo_outlier = pd.DataFrame(x_poly2_scaled_wo_outlier)\n",
    "X_poly2_scaled_wo_outlier = df_poly2_scaled_wo_outlier\n",
    "\n",
    "RegressionHelper(X_poly2_scaled_wo_outlier, y_wo_outlier_scaled, LinearRegression(), 0.2, 25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "With decision trees we try to more accurately predict what our linear regression model could not.\n",
    "Our first try was to use the decision tree regressor provided by sklearn.\n",
    "What we didn't know at first was depth of the tree and thus we overfitted quite nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree_1 = RegressionHelper(X_poly2_scaled, y_scaled, DecisionTreeRegressor(), 0.2, 25)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works really good on train data... ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "temp = plot_tree(tree_1, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding out how many different steps that tree had, we decided to limit the depth to a certain extend.\n",
    "The results of this attempt don't look too promising as well. This is why we tried to create a random forest to check if we had a better chance of predicting the case numbers with another rather similar approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree_regressor = RegressionHelper(X_poly2_scaled, y_scaled, DecisionTreeRegressor(max_depth=7), 0.2, 25)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "temp = plot_tree(tree_regressor, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Our last approach of finding a meaningful start to predict the case numbers of COVID-19 for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[rand_forest,\n",
    " rand_forest_y_test,\n",
    " rand_forest_y_pred] = RegressionHelper(X_poly2_scaled, y_scaled,\n",
    "                                        RandomForestRegressor(n_estimators=42, max_depth=5), 0.2, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Linear Regression and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "plotDataSortedByActual(ax1, lin_reg_y_test, lin_reg_y_pred, \"Linear Regression\", -0.025, 0.45)\n",
    "plotDataSortedByActual(ax2, rand_forest_y_test, rand_forest_y_pred, \"Random Forest\", -0.025, 0.45)\n",
    "\n",
    "print('                        ', 'Linear Regression', '\\t', 'Random Forest')\n",
    "print('Mean Absolute Error:    ', round(metrics.mean_absolute_error(lin_reg_y_test, lin_reg_y_pred), 4),\n",
    "     \"\\t\\t\", round(metrics.mean_absolute_error(rand_forest_y_test, rand_forest_y_pred), 4))\n",
    "print('Mean Squared Error:     ', round(metrics.mean_squared_error(lin_reg_y_test, lin_reg_y_pred), 4),\n",
    "     \"\\t\\t\", round(metrics.mean_squared_error(rand_forest_y_test, rand_forest_y_pred), 4))\n",
    "print('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(lin_reg_y_test, lin_reg_y_pred)), 4),\n",
    "     \"\\t\\t\", round(metrics.mean_squared_error(rand_forest_y_test, rand_forest_y_pred), 4))\n",
    "print('R2 Score:               ', round(metrics.r2_score(lin_reg_y_test, lin_reg_y_pred), 4),\n",
    "     \"\\t\\t\", round(metrics.r2_score(rand_forest_y_test, rand_forest_y_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest works slightly better than Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last words\n",
    "\n",
    "In the end all our results may have failed, but we believe this is because of the data.\n",
    "We simply cannot predict case numbers that are far more reliable on actual taken measures by each country than on their theoretical preparedness.\n",
    "Another important aspect of this is the authenticity of the used data, because many countries with either a poor health care system or other motives do not report accurate numbers.\n",
    "We tried to combat this with the health care expenditures, but that simply does not account for all different factors that are at play."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
